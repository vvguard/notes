# Set the batch size
batch_size = 32

# Create a DataLoader for training data
train_dataset = torch.utils.data.TensorDataset(features, labels, train_mask)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Create a DataLoader for validation data
val_dataset = torch.utils.data.TensorDataset(features, labels, val_mask)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)

# Create a DataLoader for testing data
test_dataset = torch.utils.data.TensorDataset(features, labels, test_mask)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)

# Training loop
for epoch in range(100):
    print(f'---------- epoch {epoch} ----------')
    model.train()
    total_loss = 0.0
    total_train_micro_f1 = 0.0
    total_train_macro_f1 = 0.0
    
    for batch in train_dataloader:
        batch_features, batch_labels, batch_mask = batch
        batch_features = batch_features.to(device)
        batch_labels = batch_labels.to(device)
        batch_mask = batch_mask.to(device)

        logits = model(g, batch_features)
        loss = loss_fcn(logits[batch_mask], batch_labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        train_acc, train_micro_f1, train_macro_f1 = score(logits[batch_mask], batch_labels)
        total_train_micro_f1 += train_micro_f1
        total_train_macro_f1 += train_macro_f1

    average_train_loss = total_loss / len(train_dataloader)
    average_train_micro_f1 = total_train_micro_f1 / len(train_dataloader)
    average_train_macro_f1 = total_train_macro_f1 / len(train_dataloader)

    model.eval()
    total_val_loss = 0.0
    total_val_acc = 0.0
    total_val_micro_f1 = 0.0
    total_val_macro_f1 = 0.0

    with torch.no_grad():
        for batch in val_dataloader:
            batch_features, batch_labels, batch_mask = batch
            batch_features = batch_features.to(device)
            batch_labels = batch_labels.to(device)
            batch_mask = batch_mask.to(device)

            logits = model(g, batch_features)
            val_loss = loss_fcn(logits[batch_mask], batch_labels)

            total_val_loss += val_loss.item()
            val_acc, val_micro_f1, val_macro_f1 = score(logits[batch_mask], batch_labels)
            total_val_acc += val_acc
            total_val_micro_f1 += val_micro_f1
            total_val_macro_f1 += val_macro_f1

    average_val_loss = total_val_loss / len(val_dataloader)
    average_val_acc = total_val_acc / len(val_dataloader)
    average_val_micro_f1 = total_val_micro_f1 / len(val_dataloader)
    average_val_macro_f1 = total_val_macro_f1 / len(val_dataloader)

    early_stop = stopper.step(average_val_loss, average_val_acc, model)

    print(
        "Epoch {:d} | Train Loss {:.4f} | Train Micro f1 {:.4f} | Train Macro f1 {:.4f} | "
        "Val Loss {:.4f} | Val Acc {:.4f} | Val Micro f1 {:.4f} | Val Macro f1 {:.4f}".format(
            epoch + 1,
            average_train
