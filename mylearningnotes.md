
# My learning notes

### date 9 June 2023

#### LLM and their architecture

- **cross attention** : The cross-attention helps the model to determine which parts of an input text are important for accurately predicting the next word in the output text.
- **Self-attention** : Self-attention, on the other hand, refers to a modelâ€™s ability to focus on different parts of its input when processing it.









<!-- 
# hello

- list1
* list11
- list2
+ list 3
1. one
2. two

[link text](/code2.txt)
![img](/image.jpg)

> block quote
> > blockquote


```python

print('hello')
```

- [ ] cdheck
- [x] compe 
-->
