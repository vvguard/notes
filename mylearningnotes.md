
# My learning notes

#### date 9 June 2023

### LLM and their architecture

- **cross attention** : The cross-attention helps the model to determine which parts of an input text are important for accurately predicting the next word in the output text.
- **Self-attention** : Self-attention, on the other hand, refers to a modelâ€™s ability to focus on different parts of its input when processing it.

### **Basics of transformer**

A standard transformer consists of two primary components: an encoder and a decoder, both of which rely heavily on attention mechanisms. The task of the encoder is to process the input text, identify useful features, and generate a meaningful representation of that text, known as embedding. The decoder then uses this embedding to produce an output, such as a translation or summary, that effectively interprets the encoded information. Cross-attention plays a crucial role by allowing the decoder to take advantage of the embeddings generated by the encoder. In the context of sequence-to-sequence tasks, the role of the encoder is to capture the meaning of the input text, while the role of the decoder is to generate the desired output based on the information captured by the encoder in the embedding. Together, the encoder and decoder provide a powerful tool for processing and generating text.

Encoder uses cross-attention
Decoder uses self attention

#### GPT 1

In their paper, the authors of GPT-1 proposed a new learning process where an unsupervised pre-training step is introduced. In this pre-training step, no labeled data is needed. Instead, the model is trained to predict what the next token is. Thanks to the use of the transformer architecture, which allows parallelization, this pre-training was performed on a large amount of data. 

To adapt the model to a specific target task, a second supervised learning step, called fine-tuning, was performed on a small set of manually labeled data.
- 117 million parameters

#### GPT 2
- 1.5 billion, trained on 40 GB of text. 

#### GPT 3
- 175 billion parameters
- GPT 3 also eliminates the need for a fine-tuning step that was mandatory for its predecessors.
- This verson can be toxic and wrong
- Instruct Series : Unlike the original basic GPT-3 models, the instruct models are optimized by reinforcement learning with human feedback to follow human instructions while making models more truthful and less toxic.
- The training recipe has two main stages to go from a GPT-3 model to an instructed GPT-3 model: supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). (https://arxiv.org/abs/2203.02155)


#### GPT 3.5 ChatGPT
- In November 2022, OpenAI introduced ChatGPT as an experimental conversational model. This model has been fine-tuned to excel at interactive dialogue
- The Codex series of models is a GPT-3 model fine-tuned on billions of lines of code. It powers the GitHub Copilot programming autocompletion tool to assist developers

#### GPT 4
- Unlike the other models in the OpenAI GPT family, GPT-4 is the first multimodal model capable of receiving not only text but also images.
- GPT-4 was trained with the last knowledge update in September 2021. 


### Using chat gpt

- Chat GPT playground
- Open ai api

#### How to use chatgpt effectively
- Role , context , task
- use words liek
- sub Correct this to standard English: She no went to the market.





<!-- 
# hello

- list1
* list11
- list2
+ list 3
1. one
2. two

[link text](/code2.txt)
![img](/image.jpg)

> block quote
> > blockquote


```python

print('hello')
```

- [ ] cdheck
- [x] compe 
-->
